{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87093e13-0553-486c-84bb-4939bb836234",
   "metadata": {},
   "source": [
    "## REDSEA python version for Steinbock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1218a-1221-4e70-b7e2-50a9c2909c81",
   "metadata": {},
   "source": [
    "Edited substantially and re-organized from the original REDSEA python version 0.0.1 file in this folder (see that file for details on the original python version).\n",
    "\n",
    "Edited by Ben Caiello, with the intent of allowing it to work with current DeepCell segmentation masks / the steinbock pipeline. The algorithm is not identical, as that is not possible (I think) given the differenes in the masks of old DeepCell and new DeepCell (+ / - zero boundaries changes the effective pixel width of the boundary measurement step).\n",
    "\n",
    "Dataset I had been using for CD3 / CD20 compensation: https://zenodo.org/records/8023452\n",
    "\n",
    "Passed through steinbock to derive .tiffs and masks, then fed into this script\n",
    "\n",
    "The directory structure required for the script as-written is the one naturally produced by steinbock:\n",
    "\n",
    "A master directory with two folders: /img & /masks - each containing .tiffs with the original images and the DeepCell generated masks, repectively, with matching file names - and 1 .csv file (panel.csv). These are all naturally produced by steinbock when it is run.\n",
    "\n",
    "The script has not been thoroughly tested, but should produce outputs and seems to be doing what it is supposed to with the limited testing so far.\n",
    "\n",
    "\n",
    "Some (potential and certain) differences in the algorithm to note:\n",
    "    1. Since the masks format is different with DeepCell now (no cell-cell padding with 0's), the algorithm is made to find border px by looking for >1 segmentation label (indicating >1 cell or cell + background boundary)\n",
    "    2. Also, because of the lack of padding, the effective distances and sizes of the diamond / square boder px identification step is altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9238b26c-18cd-42e1-bbff-c7d0f17f019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import PIL\n",
    "from PIL import Image, ImageSequence, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage \n",
    "import skimage.measure\n",
    "import skimage.morphology\n",
    "import glob\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "import tifffile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f14f73-9593-40d4-8621-73e87b4b6d9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Functions (under normal operations / no errors, minimize this cell to see things more easily)\n",
    "## If this actually works as intended, these should probably be moved to a separate file / turned into a mini-python library\n",
    "\n",
    "# The first two functions are from the original script\n",
    "# helper function 1\n",
    "def ismember(a, b):\n",
    "    bind = {}\n",
    "    for i, elt in enumerate(b):\n",
    "        if elt not in bind:\n",
    "            bind[elt] = i\n",
    "    return [bind.get(itm, None) for itm in a]  # None can be replaced by any other \"not in b\" value\n",
    "\n",
    "# helper function 2\n",
    "\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
    "    '''This prints a progress bar'''\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "\n",
    "## From now these are BPC edited functions\n",
    "def file_reader(steinbockPath, file_name, ends_with_tiff = False):\n",
    "    '''\n",
    "    This function reads in the panel file, the image, and its matching segmentation image from your Steinbock output folder.\n",
    "\n",
    "    args:\n",
    "    steinbockPath  -- this is the directory path to your steinbock output\n",
    "    file_name -- this is the name of the ROI / image to be read.\n",
    "    ends_with_tiff (default = False) -- this determines whether you include the .tiff suffix in the file_name. By default, do not include .tiff\n",
    "\n",
    "    returns:\n",
    "    clusterChannels -- the channels from the panel file kept by the steinbock program\n",
    "    Full_Tiff -- a multi-dimensional numpy array representing all of the channels of the .tiff image\n",
    "    Segmentation -- a 2D numpy array representing the segmentation by DeepCell\n",
    "    '''\n",
    "    ## panel file\n",
    "    if ends_with_tiff == True:\n",
    "        file_name = file_name\n",
    "    elif ends_with_tiff == False:\n",
    "        file_name = file_name + \".tiff\"\n",
    "    \n",
    "    massDS_path = steinbockPath + '/panel.csv' # csv file location, need\n",
    "    massDS = pd.read_csv(massDS_path) # read the mass csv\n",
    "    clusterChannelsDataFrame = massDS[massDS['keep'] == 1]\n",
    "    clusterChannels = clusterChannelsDataFrame.reset_index().index.values\n",
    "    \n",
    "    # Tiff file\n",
    "    pathTiff = steinbockPath + '/img/' + file_name #  tiff location, links to a single .tiff\n",
    "    array_list=[]\n",
    "    for channel in clusterChannels:\n",
    "        t=tifffile.imread(pathTiff)[channel]\n",
    "        array_list.append(t)\n",
    "    Full_Tiff=np.stack(array_list,axis=2) # count matrices in the image\n",
    "\n",
    "    ## Segmentation file\n",
    "    pathMat = steinbockPath + '/masks/' + file_name   # corresponding .tiff's mask\n",
    "    Segmentation = tifffile.imread(pathMat).astype('int')\n",
    "\n",
    "    return clusterChannelsDataFrame, Full_Tiff, Segmentation\n",
    "\n",
    "def cell_statistics(clusterChannelsDataFrame, Full_Tiff, Segmentation):\n",
    "    ''' This recovers key statistics from the files: cell / channel number, cell sizes & data, and image dimensions'''\n",
    "    clusterChannels = clusterChannelsDataFrame.reset_index().index.values\n",
    "    channelNum = len(clusterChannels) # how many channels\n",
    "    \n",
    "    cellNum = np.max(Segmentation) # how many labels\n",
    "    stats = skimage.measure.regionprops(Segmentation) # get the regional props for all the labels\n",
    "    \n",
    "    ### make empty container matrices\n",
    "    data = np.zeros((cellNum,channelNum))\n",
    "    dataScaleSize = np.zeros((cellNum,channelNum))\n",
    "    cellSizes = np.zeros((cellNum,1))\n",
    "\n",
    "    # this part extract counts data from the whole cell regions, for each individual cells etc\n",
    "    \n",
    "    for i in range(cellNum): # for each cell (label)\n",
    "        label_counts=[Full_Tiff[coord[0],coord[1],:] for coord in stats[i].coords] # all channel count for this cell\n",
    "        data[i,0:channelNum] = np.sum(label_counts, axis=0) #  sum the counts for this cell\n",
    "        dataScaleSize[i,0:channelNum] = np.sum(label_counts, axis=0) / stats[i].area # scaled by size\n",
    "        cellSizes[i] = stats[i].area # cell sizes\n",
    "    \n",
    "    [rowNum, colNum] = Segmentation.shape      #could also have been Full_Tiff.shape\n",
    "    \n",
    "    return cellNum, channelNum, data, cellSizes, dataScaleSize, rowNum, colNum\n",
    "\n",
    "\n",
    "def cell_cell_matrix(Segmentation, cellNum, rowNum, colNum,  REDSEAChecker = 1):\n",
    "    ''' Identifies cell-cell contacts, creating a matrix representing the number and identity of each cell's contact with other cells.\n",
    "\n",
    "    args:\n",
    "    Segmentation -- this is the numpy array representation of the DeepCell segmentation\n",
    "    cellNum -- this is the number of cells in the segmentation\n",
    "    rowNum -- the number of rows of pixels in the image\n",
    "    colNum -- the number of columns of pixels in the image\n",
    "    REDSEAChecker -- this determines whether the the algorithm reinforces a cell's number back to itself & compensates (1) or only removes signal with the compensation (0). Defaults to 1. \n",
    "\n",
    "    returns:\n",
    "    cellPairNorm -- this is the cell-cell contact matrix used for compensation\n",
    "    '''\n",
    "    cellPairMap = np.zeros(((cellNum + 1),(cellNum + 1))) # cell-cell shared perimeter matrix container\n",
    "    \n",
    "    # start looping the mask and produce the cell-cell contact matrix\n",
    "    for i in range(rowNum):\n",
    "        if i == 0:   # these conditional statements account for pixels on the edge of the image by shrinking the 3x3 box to smaller dimensions\n",
    "            a = 0\n",
    "            c = 2\n",
    "        elif i == (rowNum - 1):\n",
    "            a = 1\n",
    "            c = 1\n",
    "        else:\n",
    "            a = 1\n",
    "            c = 2\n",
    "        for j in range(colNum):\n",
    "            if j == 0:\n",
    "                b = 0\n",
    "                d = 2\n",
    "            elif j == (colNum - 1):\n",
    "                b = 1\n",
    "                d = 1\n",
    "            else:\n",
    "                b = 1\n",
    "                d = 2\n",
    "            tempMatrix = Segmentation[i-a:i+c,j-b:j+d] # the 3x3 window, centered on the point i,j\n",
    "            #print(tempMatrix)\n",
    "            tempFactors = np.unique(tempMatrix).astype('int') #unique\n",
    "            #print(tempFactors)\n",
    "            centerpoint_value = Segmentation[i,j]\n",
    "            #print(centerpoint_value)\n",
    "            for k in tempFactors:\n",
    "                if k != centerpoint_value: # only add to the cellPairMap for the centerpoint pixel -- this prevents multiplicate counting\n",
    "                    #print(\"trigger\")\n",
    "                    cellPairMap[centerpoint_value,k] = cellPairMap[centerpoint_value,k] + 1  \n",
    "        \n",
    "    # converting the cell cell maps to fraction of cell - cell boundary (not of total cell boundary [!?] -- as in boundary with empty space not counted)\n",
    "    cellPairNorm = np.zeros(((cellNum+1),(cellNum+1)))\n",
    "    for i in np.arange(0,len(cellPairMap)):\n",
    "        cellPairNorm[i] = - (cellPairMap[i] / np.sum(cellPairMap[i]))\n",
    "    cellPairNorm = cellPairNorm[1:,1:]\n",
    "    cellPairNorm = cellPairNorm + REDSEAChecker*np.identity(cellNum ) \n",
    "    return cellPairNorm\n",
    "\n",
    "def Cell_Edge_Intensities(Segmentation, cellNum, channelNum, Full_Tiff, rowNum, colNum, elementShape = 2, elementSize = 2):\n",
    "    '''\n",
    "    This generates the intensities of each cell's border pixels in every channel for use in the compensation\n",
    "    '''\n",
    "    MIBIdataNearEdge1 = np.zeros((cellNum+1,channelNum))\n",
    "    MIBIdataNearEdgeAvg = np.zeros((cellNum+1,channelNum))\n",
    "    #initialize progress bar:\n",
    "    items = list(range(cellNum + 1))\n",
    "    l = len(items)\n",
    "    printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 100) # progress bar\n",
    "    \n",
    "    # Square or Diamond shape of desired size\n",
    "    if elementShape==1: # square\n",
    "        shape=skimage.morphology.square((2*elementSize) + 1)\n",
    "    elif elementShape==2: # diamond\n",
    "        shape=skimage.morphology.diamond(elementSize) # create diamond shapte based on elementSize\n",
    "    else:\n",
    "        print(\"Error elementShape Value not recognized.\")   \n",
    "\n",
    "    # iterate over cells, measuring total border intensity\n",
    "    list_num_border_px = []\n",
    "    for i in range(cellNum + 1) :\n",
    "        if i == 0:\n",
    "            continue\n",
    "        num_border_px = 0\n",
    "        [tempRow,tempCol] = np.asarray(Segmentation==i).nonzero()\n",
    "        [rowNum, colNum] = Segmentation.shape\n",
    "        # sequence in row not col, should not affect the code\n",
    "        for j in range(len(tempRow)):\n",
    "            label_in_shape=[] # empty list in case\n",
    "            if (tempRow[j] - elementSize) <= -1:   # these conditionals are for ensuring the tempMatrix does not cross the image edge\n",
    "                a = 0 + tempRow[j]\n",
    "                c = 1 + elementSize\n",
    "            elif (tempRow[j] + elementSize) > rowNum - 1:\n",
    "                a = 0 + elementSize\n",
    "                c = 1 + rowNum - tempCol[j]\n",
    "            else:\n",
    "                a = 0 + elementSize\n",
    "                c = 1 + elementSize\n",
    "            if (tempCol[j] - elementSize) <= -1:\n",
    "                b = 0 + tempCol[j]\n",
    "                d = 1 + elementSize\n",
    "            elif (tempCol[j] + elementSize) > colNum - 1:\n",
    "                b = 0 + elementSize\n",
    "                d = 1 + colNum - tempCol[j]\n",
    "            else:\n",
    "                b = 0 + elementSize\n",
    "                d = 1 + elementSize\n",
    "            tempMatrix = Segmentation[tempRow[j]-a:tempRow[j]+c,tempCol[j]-b:tempCol[j]+d] # the 3x3 window, centered on the point i,j\n",
    "            if elementShape == 2:\n",
    "                [dim1, dim2] = tempMatrix.shape\n",
    "                reducedShape = shape[:dim1,:dim2].astype('bool')\n",
    "                reducedMatrix = tempMatrix[reducedShape]   # the reduced shape /  matrix lines are there to accommodate the diamond shape\n",
    "                tempFactors = np.unique(reducedMatrix).astype('int')\n",
    "            else:  # no need for reduction if using the square shape\n",
    "                tempFactors = np.unique(tempMatrix).astype('int')\n",
    "            if len(tempFactors) > 1:\n",
    "                num_border_px += 1\n",
    "                MIBIdataNearEdge1[i,:] = MIBIdataNearEdge1[i,:] + Full_Tiff[tempRow[j],tempCol[j],:]\n",
    "        MIBIdataNearEdgeAvg[i,:] =  MIBIdataNearEdge1[i,:] / num_border_px\n",
    "        list_num_border_px.append(num_border_px)\n",
    "        # Update Progress Bar\n",
    "        if ((i % 500) == 0) or (i == (cellNum - 1)):\n",
    "            printProgressBar(i + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 100)\n",
    "    MIBIdataNearEdge1 = MIBIdataNearEdge1[1:,:]\n",
    "    MIBIdataNearEdgeAvg = MIBIdataNearEdgeAvg[1:,:]\n",
    "    return MIBIdataNearEdge1, MIBIdataNearEdgeAvg, list_num_border_px\n",
    "            \n",
    "def CalculateREDSEA(MIBIdataNearEdge1, cellPairNorm, data, dataScaleSize, normChannels, clusterChannelsDataFrame, cellNum, cellSizes, Border_average = False, MIBIdataNearEdgeAvg = 0):  \n",
    "    '''\n",
    "    Takes the cell-cell matrix and border intensity matrix and calculates compensation.\n",
    "\n",
    "    args:\n",
    "    MIBIdataNearEdge1 -- cell border intensity matrix\n",
    "    cellPairNorm -- cell-cell contact matrix\n",
    "    data -- raw intensity measured for each cell in each channel\n",
    "    dataScaleSize -- the raw intensities of each normalized to cell size\n",
    "    normChannels -- the channels you wish to compensate with REDSEA\n",
    "    clusterChannelsDataFrame -- a dataframe of the channels in the dataset\n",
    "    cellNum -- the number of cell in the image\n",
    "    cellSizes -- the sizes of each cell\n",
    "    Border_average -- whether to use the raw border intensity of each cell to compensate the raw intensity of that cell (default), or the size normalized border intensity and size normalized whole cell intensity\n",
    "    MIBIdataNearEdgeAvg -- cell border intensity matrix, but with border intensities normalized by number of pixels (size of border regions). Only use with Border_average = True\n",
    "    \n",
    "    returns:\n",
    "    Four pandas dataframes, ready for export to csv's or examination in jupyter lab:\n",
    "    dataCells -- uncompensated (pre-REDSEA) cell intensity values\n",
    "    dataScaleSizeCells -- uncompensated cell intensities, normalized by cell size\n",
    "    dataCompenCells -- compensated cell intensity values\n",
    "    dataCompenScaleSizeCells -- compensated cell intensity values, normalized by cell sizes\n",
    "    '''\n",
    "    clusterChannels = clusterChannelsDataFrame.reset_index().index.values\n",
    "    normChannelsInds = ismember(normChannels,clusterChannels)\n",
    "    channelNormIdentity = np.zeros((len(clusterChannels),1))\n",
    "    # make a flag for compensation\n",
    "    for i in range(len(normChannelsInds)):\n",
    "            channelNormIdentity[normChannelsInds[i]] = 1 \n",
    "    if Border_average == False:\n",
    "        MIBIdataNorm2 = np.transpose(np.dot(np.transpose(MIBIdataNearEdge1[:,:]),cellPairNorm))\n",
    "        #this is boundary signal subtracted by cell neighbor boundary\n",
    "        MIBIdataNorm2 = MIBIdataNorm2 + data # reinforce onto the whole cell signal (original signal)\n",
    "        \n",
    "        MIBIdataNorm2[MIBIdataNorm2<0] = 0 # clear out the negative ones\n",
    "        # flip the channelNormIdentity for calculation\n",
    "        rev_channelNormIdentity=np.ones_like(channelNormIdentity)-channelNormIdentity\n",
    "        # composite the normalized channels with non-normalized channels\n",
    "        # MIBIdataNorm2 is the matrix to return\n",
    "        MIBIdataNorm2 = data * np.transpose(np.tile(rev_channelNormIdentity,(1,cellNum))) + MIBIdataNorm2 * np.transpose(np.tile(channelNormIdentity,(1,cellNum)))\n",
    "        \n",
    "        # the function should return 4 variables\n",
    "        dataCells = data\n",
    "        dataScaleSizeCells = dataScaleSize\n",
    "        dataCompenCells = MIBIdataNorm2\n",
    "        dataCompenScaleSizeCells = MIBIdataNorm2 / cellSizes\n",
    "    elif Border_average == True:\n",
    "        MIBIdataNorm2 = np.transpose(np.dot(np.transpose(MIBIdataNearEdgeAvg),cellPairNorm))\n",
    "        MIBIdataNorm2 = MIBIdataNorm2 + dataScaleSize\n",
    "        MIBIdataNorm2[MIBIdataNorm2<0] = 0\n",
    "        rev_channelNormIdentity=np.ones_like(channelNormIdentity)-channelNormIdentity\n",
    "        MIBIdataNorm2 = data * np.transpose(np.tile(rev_channelNormIdentity,(1,cellNum))) + (MIBIdataNorm2 * np.transpose(np.tile(channelNormIdentity,(1,cellNum))) * cellSizes)\n",
    "        dataCells = dataScaleSize * cellSizes\n",
    "        dataScaleSizeCells = dataScaleSize\n",
    "        dataCompenCells = MIBIdataNorm2 \n",
    "        dataCompenScaleSizeCells = MIBIdataNorm2 / cellSizes\n",
    "    list_of_arrays = [dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells]\n",
    "    for jj,j in enumerate(list_of_arrays):\n",
    "        labelVec = [i for i in np.arange(1,cellNum + 1,1)]\n",
    "        cellSizesVec_flat = [item for sublist in cellSizes for item in sublist] # flat the list\n",
    "        dataL = pd.DataFrame({'Object':labelVec})\n",
    "        dataCells_df=pd.DataFrame(j)\n",
    "        list_of_columns = []\n",
    "        for i in clusterChannelsDataFrame[\"name\"]:\n",
    "            list_of_columns.append(i)\n",
    "        dataCells_df.columns = list_of_columns\n",
    "        if jj == 0:\n",
    "            dataCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "        if jj == 1:\n",
    "            dataScaleSizeCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "        if jj == 2:\n",
    "            dataCompenCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "        if jj == 3:\n",
    "            dataCompenScaleSizeCells = pd.concat((dataL,dataCells_df),axis=1)\n",
    "    return dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells\n",
    "\n",
    "def Outputter(data_in, file_name, pathResults):\n",
    "    data_in.to_csv(pathResults + '/' + file_name + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827bda19-7c0a-4d81-b25e-87cd83c76a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Edit these for your run!\n",
    "# file locations\n",
    "steinbockPath = 'C:/Users/You/The/File/Path/To/Steinbock' # main folder must contain /img and /masks folders and a panel.csv file (matching steinbock output)\n",
    "\n",
    "# or set file_name manually\n",
    "file_name = 'RNANeg_Tonsil_003'  #do not include the .tiff suffix! Or, alternatively, include .tiff, but set the ends_with_tiff argument in the file_reader function to True!\n",
    "\n",
    "#  select which channel to normalize\n",
    "normChannels = [13,18]  # Note that this only works with channels entered as zero-indexed numbers -- count in your panel file's order and open an img in napari to see index  \n",
    "# However, at the ouput, the file will contain the matching channel label (like 89Y-CD3, or something like that) and not just the channel number.",
    "\n",
     "# Be very careful with the zero-indexing or you will RedSEA the wrong channels (!!)\n", 
    "\n",
    "# parameters for compensation (change as desired)\n",
    "REDSEAChecker = 1 # 1 means subtract + reinforce\n",
    "elementShape = 1 # star == 2, square == 1\n",
    "elementSize = 1 # star or square expansion size\n",
    "\n",
    "# output path\n",
    "pathResults = steinbockPath + '/intensities' # output location, set as intensities here to match steinbock output\n",
    "try:\n",
    "    os.listdir(pathResults)\n",
    "except:\n",
    "    os.mkdir(pathResults)\n",
    "\n",
    "alt_out_path = \"/an/alternate/folder/in/which/to/put/your/non-scaled/and/non-compensated/data\" ## use if you want the non-compensated / non-scaled data, but don't want them crowding the main steinbock intensities folder\n",
    "######### if you use alt_path, manually make the directory in file explorer!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5bb310-9f55-4551-a9d0-89f65c7fbf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ-| 100.0% Complete\r"
     ]
    }
   ],
   "source": [
    "## Run the Program! Single file run:\n",
    "clusterChannelsDataFrame, Full_Tiff, Segmentation = file_reader(steinbockPath, file_name)\n",
    "cellNum, channelNum, data, cellSizes, dataScaleSize, rowNum, colNum = cell_statistics(clusterChannelsDataFrame, Full_Tiff, Segmentation)\n",
    "cellPairNorm = cell_cell_matrix(Segmentation, cellNum, rowNum, colNum,  REDSEAChecker = 1)\n",
    "MIBIdataNearEdge1, MIBIdataNearEdgeAvg, list_num_border_px = Cell_Edge_Intensities(Segmentation, cellNum, channelNum, Full_Tiff, rowNum, colNum, elementShape = elementShape, elementSize = elementSize)\n",
    "dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells = CalculateREDSEA(MIBIdataNearEdge1, cellPairNorm, data, dataScaleSize, normChannels, clusterChannelsDataFrame, cellNum, cellSizes, Border_average = True, MIBIdataNearEdgeAvg = MIBIdataNearEdgeAvg)\n",
    "\n",
    "Outputter(dataScaleSizeCells, file_name = file_name + \"_pre_REDSEA_scaled\", pathResults = pathResults)\n",
    "Outputter(dataCompenScaleSizeCells, file_name, pathResults = pathResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce749e62-0b97-4baf-aab5-3ef69d636233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RedSEA compensation of file: RNANeg_Tonsil_001\n",
      "Starting RedSEA compensation of file: RNANeg_Tonsil_002â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ-| 100.0% Complete\n",
      "Starting RedSEA compensation of file: RNANeg_Tonsil_003â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ-| 100.0% Complete\n",
      "Starting RedSEA compensation of file: RNANeg_Tonsil_004â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ-| 100.0% Complete\n",
      "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ-| 100.0% Complete\r"
     ]
    }
   ],
   "source": [
    "## Run the Program! Whole folder of images at once!\n",
    "file_list = []\n",
    "for i in os.listdir(steinbockPath + \"/img\"):\n",
    "    i = i.replace(\".tiff\",\"\")\n",
    "    file_list.append(i) \n",
    "\n",
    "for i in file_list:           # iterate over files in directory\n",
    "    file_name = i\n",
    "    print(\"Starting RedSEA compensation of file: \" + file_name)\n",
    "    clusterChannelsDataFrame, Full_Tiff, Segmentation = file_reader(steinbockPath, file_name)\n",
    "    cellNum, channelNum, data, cellSizes, dataScaleSize, rowNum, colNum = cell_statistics(clusterChannelsDataFrame, Full_Tiff, Segmentation)\n",
    "    cellPairNorm = cell_cell_matrix(Segmentation, cellNum, rowNum, colNum,  REDSEAChecker = 1)\n",
    "    MIBIdataNearEdge1, MIBIdataNearEdgeAvg, list_num_border_px = Cell_Edge_Intensities(Segmentation, cellNum, channelNum, Full_Tiff, rowNum, colNum, elementShape = elementShape, elementSize = elementSize)\n",
    "    dataCells, dataScaleSizeCells, dataCompenCells, dataCompenScaleSizeCells = CalculateREDSEA(MIBIdataNearEdge1, cellPairNorm, data, dataScaleSize, normChannels, clusterChannelsDataFrame, cellNum, cellSizes, Border_average = False, MIBIdataNearEdgeAvg = 0)\n",
    "    ################ Change / add Outputter calls if you want to write dataframes besides the scaled cells and compensated scaled cells:  \n",
    "    Outputter(dataScaleSizeCells, file_name = file_name + \"_pre_REDSEA_scaled\", pathResults = pathResults)\n",
    "    Outputter(dataCompenScaleSizeCells, file_name, pathResults = pathResults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae2749b-c998-4baa-ba1c-02cd8e39116e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>0.483871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7215</th>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7216</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7217 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ratio\n",
       "0     0.500000\n",
       "1     0.488372\n",
       "2     0.583333\n",
       "3     0.481481\n",
       "4     0.395833\n",
       "...        ...\n",
       "7212  1.000000\n",
       "7213  0.483871\n",
       "7214  1.000000\n",
       "7215  0.875000\n",
       "7216  0.666667\n",
       "\n",
       "[7217 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ratio    0.444672\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### This cell I've been using for testing aspects of the code / data\n",
    "\n",
    "######### This little code block prints the ratio of border pixels to total cell size in each cell\n",
    "## With my data I was seeing cells that looked like they were ALL border pixels -- which kind of ruins the point of only selecting border pixel intensities\n",
    "## If you see this effect, consider reducing the elementSize parameter / using the diamond elementShape (elementShape = 2) to try to classify fewer pixels as border pixels\n",
    "border_px_percent = []\n",
    "for i in range(len(cellSizes)):\n",
    "    border_px_percent.append(list_num_border_px[i] / cellSizes[i])\n",
    "cell_border_percent_df = pd.DataFrame(border_px_percent, columns = [\"ratio\"])\n",
    "display(cell_border_percent_df)\n",
    "display(cell_border_percent_df.mean())\n",
    "\n",
    "#dataCompenScaleSizeCells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c440d-1b1d-4b24-a279-97bcf883532c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
